WEBVTT
Kind: captions
Language: zh-Hans

00:00:00.000 --> 00:00:02.400
大家好这里是最佳拍档我是大飞

00:00:02.700 --> 00:00:03.800
这半年时间啊

00:00:03.800 --> 00:00:06.000
大语言模型无疑是最火爆的

00:00:06.100 --> 00:00:08.833
但是我们呢一直没有好好的去讲一下

00:00:08.966 --> 00:00:11.633
大语言模型内部究竟是如何工作的

00:00:12.166 --> 00:00:12.966
不过最近啊

00:00:13.033 --> 00:00:14.633
蒂姆·李(Tim Lee)和肖恩·特洛特(Sean Trott)

00:00:14.633 --> 00:00:16.233
联合编写了一篇文章

00:00:16.400 --> 00:00:18.666
用最少的数学知识和术语

00:00:18.766 --> 00:00:20.700
对大语言模型进行了解释

00:00:21.100 --> 00:00:23.433
先简单对文章作者做一下介绍啊

00:00:23.800 --> 00:00:27.200
蒂姆·李曾经任职于科技媒体Ars Technica

00:00:27.400 --> 00:00:29.500
他最近呢也推出了一份newsletter

00:00:29.833 --> 00:00:30.633
《Understanding AI》

00:00:30.833 --> 00:00:33.200
主要是探讨人工智能的工作原理

00:00:33.533 --> 00:00:34.666
而肖恩特洛特呢

00:00:34.666 --> 00:00:37.800
是加里福尼亚大学圣迭戈分校的助理教授

00:00:38.433 --> 00:00:41.233
主要研究人类语言理解和语言模型

00:00:41.600 --> 00:00:43.900
好了以下是我翻译的文章内容

00:00:44.000 --> 00:00:46.066
咱们看看当你看完视频之后

00:00:46.233 --> 00:00:49.066
究竟能否理解大语言模型的内部机制

00:00:49.500 --> 00:00:53.333
全文呢几乎没有太复杂的数学概念、公式和运算

00:00:53.866 --> 00:00:54.500
所以我觉得呢

00:00:54.500 --> 00:00:57.433
对于很多初学者来说也是非常友好的

00:00:57.633 --> 00:01:00.366
当ChatGPT在去年秋天推出的时候

00:01:00.500 --> 00:01:04.000
在科技行业乃至全世界的范围内引起了轰动

00:01:04.600 --> 00:01:06.600
当时呢机器学习的研究人员

00:01:06.600 --> 00:01:08.966
已经研发了多年的大语言模型

00:01:09.200 --> 00:01:11.700
但是普通大众并没有十分的关注

00:01:12.033 --> 00:01:14.366
也没有意识到他们会变得有多强大

00:01:14.666 --> 00:01:17.999
如今呢几乎每个人都听说过大语言模型了

00:01:18.166 --> 00:01:20.600
并且呢有数千万人用过他们

00:01:20.800 --> 00:01:23.700
但是了解他们工作原理的人并不多

00:01:24.266 --> 00:01:25.300
你可能听说过

00:01:25.300 --> 00:01:28.500
训练大语言模型是用来预测下一个词

00:01:28.700 --> 00:01:31.633
而且呢他们需要大量的文本来实现这一点

00:01:32.066 --> 00:01:35.366
但是一般的解释呢通常也就是止步于此

00:01:35.766 --> 00:01:38.300
他们究竟如何预测下一个词的细节

00:01:38.300 --> 00:01:40.866
往往被大家视为一个深奥的谜题

00:01:41.233 --> 00:01:42.600
其中一个原因是

00:01:42.666 --> 00:01:45.766
大语言模型的开发方式非常与众不同

00:01:46.366 --> 00:01:49.299
一般的软件呢都是由人类工程师所编写的

00:01:49.466 --> 00:01:52.566
他们为计算机提供明确的逐步的指令

00:01:53.000 --> 00:01:54.033
而相比之下

00:01:54.100 --> 00:01:55.900
ChatGPT是建立在一个

00:01:55.900 --> 00:01:58.000
使用了数十亿个语言词汇

00:01:58.266 --> 00:02:00.400
进行训练的神经网络之上

00:02:00.600 --> 00:02:02.100
因此呢到现在为止

00:02:02.233 --> 00:02:04.233
地球上也没有人完全理解

00:02:04.233 --> 00:02:06.400
大语言模型的内部工作原理

00:02:07.100 --> 00:02:10.000
研究人员正在努力尝试理解这些模型

00:02:10.166 --> 00:02:11.966
但是这是一个需要数年

00:02:11.966 --> 00:02:14.866
甚至几十年才能够完成的缓慢过程

00:02:15.066 --> 00:02:18.299
不过呢专家们确实对这些系统的工作原理

00:02:18.300 --> 00:02:19.933
已经有了不少的了解

00:02:20.066 --> 00:02:23.466
我们的目的呢是将这些知识开放给广大的受众

00:02:23.766 --> 00:02:27.366
我们将在不涉及技术术语或者高级数学的前提下

00:02:27.566 --> 00:02:30.800
努力解释已知的大语言模型内部的工作原理

00:02:31.233 --> 00:02:34.066
我们将从解释词向量Word Vector开始

00:02:34.333 --> 00:02:38.733
这是语言模型表示和推理语言的一种令人惊讶的方式

00:02:39.033 --> 00:02:41.533
然后我们将深入探讨Transformer

00:02:41.766 --> 00:02:44.200
它是构建ChatGPT等模型的基石

00:02:44.433 --> 00:02:48.066
最后呢我们将解释这些模型是如何训练的

00:02:48.100 --> 00:02:51.200
并且探讨为什么要使用庞大的数据量

00:02:51.200 --> 00:02:53.233
才能够获得良好的性能

00:02:53.766 --> 00:02:55.833
要了解语言模型的工作原理

00:02:55.833 --> 00:02:58.866
首先需要了解他们如何来表示单词

00:02:59.433 --> 00:03:02.066
人类呢是用字母序列来表示英文单

00:03:02.066 --> 00:03:05.500
词的比如说C-A-T cat表示猫

00:03:05.900 --> 00:03:09.066
而语言模型呢使用的是一个叫做词向量的

00:03:09.066 --> 00:03:11.033
一长串数字的列表

00:03:11.200 --> 00:03:14.533
比如说这是一种将猫表示为向量的方式

00:03:14.866 --> 00:03:17.800
完整的向量长度呢实际上有300个数字

00:03:18.266 --> 00:03:21.166
那为什么要用这么复杂的表示方法呢

00:03:21.400 --> 00:03:22.933
这里边啊有个类比

00:03:23.033 --> 00:03:28.199
比如说华盛顿区位于北纬38.9度西经77度

00:03:28.400 --> 00:03:30.433
我们可以用向量表示法表示为

00:03:30.633 --> 00:03:34.433
华盛顿区的坐标是38.9和77

00:03:34.766 --> 00:03:37.300
纽约的坐标呢是40.7和74

00:03:37.566 --> 00:03:40.533
伦敦的坐标呢是51.5和0.1

00:03:40.733 --> 00:03:43.800
巴黎的坐标呢是48.9和-2.4

00:03:43.966 --> 00:03:46.333
这对于推理空间关系很有用

00:03:46.466 --> 00:03:49.366
你可以看出纽约离华盛顿区很近

00:03:49.533 --> 00:03:52.966
因为坐标中的38.9接近于40.7

00:03:53.066 --> 00:03:55.533
而77呢接近于74

00:03:56.033 --> 00:03:58.533
同样呢巴黎离伦敦也很近

00:03:58.666 --> 00:04:01.166
但是巴黎离华盛顿区很远

00:04:01.600 --> 00:04:04.333
大语言模型呢正是采用了类似的方法

00:04:04.766 --> 00:04:08.466
每个词向量代表了词空间word space中的一个点

00:04:08.666 --> 00:04:12.266
具有相似含义的词的位置互相会更为接近

00:04:12.633 --> 00:04:14.566
比如说在向量空间中

00:04:14.566 --> 00:04:19.200
与猫cat最接近的词就包括dog、kitten和pet

00:04:19.766 --> 00:04:22.433
用实数向量来表示像cat这样的单词

00:04:22.600 --> 00:04:24.266
它的一个主要优点就是

00:04:24.266 --> 00:04:27.200
数字能够进行字母无法进行的运算

00:04:27.866 --> 00:04:29.400
单词太过于复杂了

00:04:29.400 --> 00:04:31.566
无法只使用二维来表示

00:04:31.733 --> 00:04:36.333
因此大语言模型使用了具有数百甚至数千维度的向量空间

00:04:36.866 --> 00:04:39.866
人们无法想象具有如此高维度的空间

00:04:39.966 --> 00:04:42.766
但是计算机完全可以对它进行推理

00:04:42.833 --> 00:04:44.300
并产生有用的结果

00:04:45.033 --> 00:04:48.033
几十年来研究人员一直在研究词向量

00:04:48.266 --> 00:04:51.833
但是这个概念呢真正引起关注的是在2013年

00:04:51.966 --> 00:04:54.700
那时候Google公布了word2vec项目

00:04:54.833 --> 00:04:58.299
Google分析了从Google新闻中收集的数百万篇文档

00:04:58.566 --> 00:05:02.433
为了找出哪些单词倾向于出现在相似的句子中

00:05:03.066 --> 00:05:04.266
随着时间的推移

00:05:04.266 --> 00:05:06.066
一个经过训练的神经网络

00:05:06.066 --> 00:05:08.433
学会了将相似类别的单词

00:05:08.466 --> 00:05:10.133
比如说dog和cat

00:05:10.366 --> 00:05:12.733
放置在向量空间中的相邻位置

00:05:13.166 --> 00:05:15.866
Google的词向量还具有另一个有趣的特点

00:05:16.066 --> 00:05:18.933
你可以使用向量运算来推理单词

00:05:19.366 --> 00:05:20.166
比如说

00:05:20.166 --> 00:05:23.033
Google研究人员取出biggest的向量

00:05:23.100 --> 00:05:26.100
减去big的向量再加上small的向量

00:05:26.200 --> 00:05:28.966
与结果向量最接近的词就是smallest

00:05:29.566 --> 00:05:30.366
也就是说

00:05:30.400 --> 00:05:32.833
你可以使用向量运算来进行类比

00:05:33.066 --> 00:05:34.066
在这个例子中

00:05:34.133 --> 00:05:38.733
big与biggest的关系类似于small与smallest的关系

00:05:38.900 --> 00:05:41.900
Google的词像量还捕捉到了许多其他的关系

00:05:42.066 --> 00:05:46.900
比方说瑞士人与瑞士这类似于柬埔寨人与柬埔寨

00:05:47.333 --> 00:05:50.233
巴黎于法国类似于柏林与德国

00:05:50.433 --> 00:05:54.366
不道德的与道德的类似于可能的与不可能的

00:05:54.433 --> 00:05:57.466
mouse与mice类似于dollar与dollars

00:05:57.933 --> 00:06:01.133
男人与女人类似于国王与女王

00:06:01.466 --> 00:06:02.366
等等等等

00:06:02.533 --> 00:06:05.966
因为这些向量是从人们使用语言的方式中构建的

00:06:06.200 --> 00:06:09.900
所以他们反映了许多存在于人类语言中的偏见

00:06:10.133 --> 00:06:12.700
比如说在某些词项链的模型中

00:06:12.766 --> 00:06:16.066
医生减去男人再加上女人等于护士

00:06:16.566 --> 00:06:19.533
减少这种偏见呢是一个很新颖的研究领域

00:06:19.933 --> 00:06:20.866
尽管如此

00:06:20.866 --> 00:06:23.333
词向量是大语言模型的一个基础

00:06:23.533 --> 00:06:27.933
他们编码了词与词之间微妙但是重要的关系信息

00:06:28.366 --> 00:06:31.866
如果一个大语言模型学到了关于cat的一些知识

00:06:32.100 --> 00:06:34.733
比方说他有时候会去看兽医

00:06:34.900 --> 00:06:38.733
那同样的事情呢很可能也适用于kitten或者dog

00:06:39.333 --> 00:06:42.966
如果模型学到了关于巴黎和法国之间的关系

00:06:43.066 --> 00:06:45.700
比方说他们使用了同一种语言

00:06:45.966 --> 00:06:49.399
那么柏林和德国以及罗马和意大利的关系

00:06:49.533 --> 00:06:51.000
很可能也是一样的

00:06:51.133 --> 00:06:53.900
但是像这样简单的词向量方案

00:06:53.900 --> 00:06:56.866
并没有捕获到自然语言的一个重要事实

00:06:57.100 --> 00:06:59.700
那就是一个单词通常有多重的含义

00:07:00.066 --> 00:07:00.700
比如说

00:07:00.700 --> 00:07:04.133
单词bank可以指金融机构或者是河岸

00:07:04.466 --> 00:07:06.233
或者以这两个句子为例

00:07:06.400 --> 00:07:10.200
在这两个句子中magazine的含义相关但是又有不同

00:07:10.366 --> 00:07:12.133
约翰拿起的是一本杂志

00:07:12.200 --> 00:07:15.066
而苏珊为一家出版杂志的机构工作

00:07:15.466 --> 00:07:17.700
当一个词有两个无关的含义时

00:07:17.866 --> 00:07:21.700
语言学家称之为同音异义词（homonyms）

00:07:21.833 --> 00:07:24.466
当一个词有两个紧密相关的意义时

00:07:24.500 --> 00:07:25.733
比如说这个magazine

00:07:25.833 --> 00:07:28.633
语言学家呢称之为多义词（polysemy）

00:07:29.233 --> 00:07:31.366
像ChatGPT这样的大语言模型

00:07:31.366 --> 00:07:33.600
能够根据单词出现的上下文

00:07:33.633 --> 00:07:36.033
用不同的向量来表示同一个词

00:07:36.400 --> 00:07:39.100
有一个针对于机构的bank的向量

00:07:39.300 --> 00:07:42.000
还有一个针对于河岸的bank的向量

00:07:42.166 --> 00:07:44.566
有一个针对于杂志的magazine的向量

00:07:44.633 --> 00:07:47.599
还有一个针对于杂志社的magazine的向量

00:07:47.866 --> 00:07:49.266
对于多义词的含义啊
正如你预想的那样

00:07:49.266 --> 00:07:51.800
大语言模型使用的向量会更相似

00:07:52.033 --> 00:07:54.233
而对于同音异义词的含义

00:07:54.266 --> 00:07:56.233
使用的向量呢则不太相似

00:07:56.433 --> 00:07:57.400
到目前为止

00:07:57.400 --> 00:08:00.433
我们还没有解释语言模型是如何做到这一点的

00:08:00.566 --> 00:08:02.500
我们很快呢会进入这个话题

00:08:02.633 --> 00:08:05.600
不过详细说明这些向量表示

00:08:05.600 --> 00:08:08.966
这对于理解大语言模型的工作原理非常重要

00:08:09.400 --> 00:08:12.833
在传统软件的设计中数据处理呢是明确的

00:08:13.233 --> 00:08:15.766
比如说你让计算机计算2+3

00:08:15.933 --> 00:08:20.133
关于2、加号或者3的含义呢都不存在歧义问题

00:08:20.300 --> 00:08:22.133
但是自然语言中的歧义

00:08:22.300 --> 00:08:25.000
远不止于同音异义词和多义词

00:08:25.133 --> 00:08:28.233
比方说顾客请修理工修理他的车

00:08:28.400 --> 00:08:32.000
这句话中his是指顾客还是指修理工

00:08:32.666 --> 00:08:35.666
教授催促学生完成她的家庭作业中

00:08:35.933 --> 00:08:38.066
her是指教授还是学生

00:08:38.500 --> 00:08:39.866
第三句中的flies

00:08:40.200 --> 00:08:42.233
到底是一个动词在空中飞

00:08:42.233 --> 00:08:44.100
还是一个名词果蝇呢

00:08:44.366 --> 00:08:47.733
在现实中人们会根据上下文来解决这类歧义

00:08:47.833 --> 00:08:50.433
但是并没有一个简单或者明确的规则

00:08:50.800 --> 00:08:54.433
相反呢这就需要理解关于这个世界的实际情况

00:08:54.800 --> 00:08:55.600
你需要知道

00:08:55.600 --> 00:08:58.066
修理工经常会修理顾客的汽车

00:08:58.400 --> 00:09:00.966
学生呢通常会完成自己的家庭作业

00:09:01.066 --> 00:09:03.066
而水果呢通常不会飞

00:09:03.466 --> 00:09:07.533
因此呢词向量为大语言模型提供了一种灵活的方式

00:09:07.766 --> 00:09:10.000
用来在特定段落的上下文中

00:09:10.000 --> 00:09:12.066
表示每个词的准确含义

00:09:12.233 --> 00:09:13.266
现在让我们来看看

00:09:13.266 --> 00:09:15.433
他们是究竟如何做到这一点的

00:09:15.833 --> 00:09:18.300
ChatGPT最初版本背后的GPT-3

00:09:18.300 --> 00:09:21.566
模型是由数十个神经网络层组成的

00:09:21.966 --> 00:09:25.133
因为输入文本中的每个词会对应着一个向量

00:09:25.300 --> 00:09:27.533
所以这些神经网络中的每一层

00:09:27.666 --> 00:09:30.266
都会接受一系列的向量作为输入

00:09:30.433 --> 00:09:33.699
并添加一些信息来帮助澄清这个词的含义

00:09:33.933 --> 00:09:36.833
从而更好的预测接下来可能出现的词

00:09:37.300 --> 00:09:39.533
让我们从一个简单的示例说起

00:09:40.066 --> 00:09:42.700
大语言模型的每个层呢都是一个Transformer

00:09:43.066 --> 00:09:46.300
2017年Google在一篇里程碑式的论文中

00:09:46.333 --> 00:09:48.533
首次介绍了这种神经网络结构

00:09:48.666 --> 00:09:49.700
在这张图的底部

00:09:49.766 --> 00:09:53.899
模型的输入文本是John wants his back to catch   the

00:09:54.100 --> 00:09:56.866
翻译过来就是约翰想让他的银行兑现

00:09:57.000 --> 00:10:00.366
这些单词呢被表示为word2vec的风格的向量

00:10:00.366 --> 00:10:02.266
并传提给第一个Transformer

00:10:02.733 --> 00:10:05.933
这个Transformer确定了wants和cash都是动词

00:10:06.266 --> 00:10:10.066
我们用小括号内的红色文本表示这个附加的上下文

00:10:10.500 --> 00:10:15.166
但实际上模型会通过修改词向量的方式来存储这个信息

00:10:15.333 --> 00:10:17.866
这种方式对于人类来说很难解释

00:10:18.333 --> 00:10:22.033
这些新的向量被称为隐藏状态hidden state

00:10:22.433 --> 00:10:24.333
并传递给下一个Transformer

00:10:24.500 --> 00:10:27.566
第二个transformer添加了另外两个上下文信息

00:10:27.766 --> 00:10:31.633
他澄清了bank是金融机构financial institution

00:10:31.933 --> 00:10:33.033
而不是河岸

00:10:33.200 --> 00:10:35.766
并且his是指代John的代词

00:10:36.366 --> 00:10:39.500
第二个Transformer产生了另一组隐藏状态向量

00:10:39.766 --> 00:10:43.900
这组向量反映的是这个模型之前所学习的所有信息

00:10:44.133 --> 00:10:47.866
这张图表描绘的是一个纯粹假想的大语言模型

00:10:47.966 --> 00:10:50.266
所以大家呢不要对细节过于较真

00:10:50.633 --> 00:10:53.166
真实的大圆模型往往有更多的层

00:10:53.500 --> 00:10:57.033
比如说最强大的GPT-3版本有96层

00:10:57.733 --> 00:10:58.666
有研究表明

00:10:58.733 --> 00:11:02.633
前几层的神经网络会专注于理解句子的语法

00:11:02.733 --> 00:11:05.100
并且解决上面所表示的歧义

00:11:05.366 --> 00:11:09.833
而后面的层则致力于对整个文本段落的高层次的理解

00:11:10.366 --> 00:11:14.366
比如说当大语言模型阅读一篇短篇小说的时候

00:11:14.666 --> 00:11:17.766
他似乎会记住关于故事角色的各种信息

00:11:17.933 --> 00:11:21.166
包括性别和年龄、与其他角色的关系

00:11:21.300 --> 00:11:24.766
过去和当前的位置个性和目标等等

00:11:25.000 --> 00:11:26.900
研究人员呢并不完全了解

00:11:26.900 --> 00:11:29.400
大语言模型是如何跟踪这些信息的

00:11:29.533 --> 00:11:31.100
但是从逻辑上来讲

00:11:31.166 --> 00:11:33.800
模型在各层之间传递信息时候

00:11:33.933 --> 00:11:36.833
必须通过修改隐藏状态的向量来实现

00:11:37.033 --> 00:11:39.733
现代大语言模型中的向量维度极为庞大

00:11:39.866 --> 00:11:42.933
这有利于表达更为丰富的语义信息

00:11:43.400 --> 00:11:49.600
比如说GPT-3最强大的版本使用了有12,288个维度的词向量

00:11:49.933 --> 00:11:55.966
也就是说每个词是由一个包含了12,288个的数字序列表示

00:11:56.266 --> 00:12:00.466
这比Google在2013年提出的word2vec的方案要大20倍

00:12:01.033 --> 00:12:03.999
你可以把所有这些额外的维度看作是GPT-3

00:12:04.000 --> 00:12:09.266
可以用来记录每个词的上下文的一种暂存空间Scratch space

00:12:09.500 --> 00:12:13.800
较早的层所做的信息笔记可以被后来的层读取和修改

00:12:13.900 --> 00:12:17.066
从而使得模型逐渐加深对整篇文章的理解

00:12:17.500 --> 00:12:22.900
因此假设我们将之前的图表改为描述一个96层的语言模型

00:12:22.966 --> 00:12:24.966
来解读一个1,000字的故事

00:12:25.200 --> 00:12:28.433
那么第60层可能会包含一个用于John的向量

00:12:28.633 --> 00:12:33.800
带有一个表示为主角、男性、娶了谢利尔唐、纳德的表弟

00:12:33.800 --> 00:12:39.333
来自于明尼斯达州、目前在博伊希、试图找到他丢失的钱包

00:12:39.333 --> 00:12:41.500
这样一整套的括号注释

00:12:42.033 --> 00:12:44.400
同样呢所有这些以及更多的事实

00:12:44.400 --> 00:12:49.533
都会以一个包含12,288个数字列表的形式进行编码

00:12:49.900 --> 00:12:52.500
这些数字都对应着这个词John

00:12:52.866 --> 00:12:54.933
或者说这个故事中的其他词

00:12:54.933 --> 00:12:58.266
比方说谢利尔、唐纳德、伯伊希、钱包

00:12:58.266 --> 00:12:59.533
或者是其他的词

00:12:59.933 --> 00:13:05.033
他们的某些信息也会被编码在12,288维的向量中

00:13:05.533 --> 00:13:09.400
这样做的目标是让网络的第96层和最后一层

00:13:09.400 --> 00:13:12.666
输出一个包含所有必要信息的隐藏状态

00:13:12.833 --> 00:13:14.700
从而来预测下一个单词

00:13:15.466 --> 00:13:18.700
现在我们来谈谈每个Transformer内部发生的情况

00:13:19.133 --> 00:13:23.100
Transformer在更新输入段落的每个单词的隐藏状态时候

00:13:23.133 --> 00:13:24.466
有两个处理过程

00:13:24.733 --> 00:13:26.900
第一个呢是在注意力的步骤中

00:13:27.033 --> 00:13:28.666
词汇会观察周围

00:13:28.866 --> 00:13:33.066
查找具有相关背景并彼此共享信息的其他的词

00:13:33.200 --> 00:13:34.933
第二呢在前馈步骤中

00:13:35.033 --> 00:13:38.800
每个词会思考之前注意力步骤中收集到的信息

00:13:38.933 --> 00:13:40.800
并尝试预测下一个词

00:13:41.266 --> 00:13:44.100
当然了执行这些步骤的是整个网络

00:13:44.100 --> 00:13:45.600
而不是个别的单词

00:13:45.900 --> 00:13:48.400
但是我们用这种方式来表述是为了强调

00:13:48.400 --> 00:13:52.433
Transformer是以单词作为这一个分析的基本单元

00:13:52.633 --> 00:13:54.866
而不是整个句子或者是段落

00:13:55.200 --> 00:13:58.433
这种方法使得大语言模型能够充分的利用

00:13:58.433 --> 00:14:01.333
现代GPU芯片的大规模并行处理能力

00:14:01.900 --> 00:14:03.533
它还可以帮助大语言模型

00:14:03.533 --> 00:14:06.433
扩展到包含成千上万个词的长段落

00:14:06.800 --> 00:14:10.400
而这两个方面都是早期大语言模型所面临的挑战

00:14:10.566 --> 00:14:11.866
你可以将注意力机制

00:14:11.866 --> 00:14:14.333
看作是单词之间的一个撮合服务

00:14:14.500 --> 00:14:18.166
每个单词呢都会制作一个检查表称为查询向量

00:14:18.566 --> 00:14:20.733
来描述他寻找的词的特征

00:14:21.100 --> 00:14:24.500
每个词呢还会制作一个检查表称为关键向量

00:14:24.500 --> 00:14:25.966
描述他自己的特征

00:14:26.333 --> 00:14:30.599
神经网络通过将每个关键向量与每个查询向量进行比较

00:14:31.000 --> 00:14:34.500
通过计算他们的点积来找到最佳匹配的单词

00:14:34.966 --> 00:14:36.566
一旦找到匹配项

00:14:36.566 --> 00:14:39.066
他就会从产生关键向量的单词

00:14:39.266 --> 00:14:42.899
把相关信息传递给产生查询向量的单词

00:14:43.266 --> 00:14:45.400
比如说在前面的部分中

00:14:45.666 --> 00:14:48.266
我们展示了一个假想的Transformer模型

00:14:48.400 --> 00:14:52.266
他发现在“John wants his bank to cash the”这个句子中

00:14:52.366 --> 00:14:53.933
his指的就是John

00:14:54.366 --> 00:14:55.433
在系统内部

00:14:55.433 --> 00:14:56.833
过程可能是这个样子

00:14:57.033 --> 00:15:00.233
his的查询向量可能会有效的表示为

00:15:00.233 --> 00:15:03.599
我正在寻找一名描述男性的名词

00:15:03.866 --> 00:15:06.966
而John的关键向量可能会有效的表述为

00:15:06.966 --> 00:15:09.166
我是一个描述男性的名词

00:15:09.400 --> 00:15:12.666
然后网络就会检测到这两个向量是匹配的

00:15:12.733 --> 00:15:16.399
并将关于John的向量信息转移给his的向量

00:15:16.866 --> 00:15:19.400
每个注意力层都有几个注意力头

00:15:19.533 --> 00:15:24.633
这意味着这个信息交换的过程在每一层上会并行的进行多次

00:15:24.633 --> 00:15:27.933
每个注意力头呢都会专注于不同的任务

00:15:28.200 --> 00:15:30.166
比方说其中一个注意力头

00:15:30.166 --> 00:15:32.833
可能会将代词与名词进行匹配

00:15:33.000 --> 00:15:34.066
另外一个注意力头

00:15:34.066 --> 00:15:37.666
可能会处理解析类似于bank这样的一词多义的含义

00:15:37.833 --> 00:15:38.733
第三个注意力头

00:15:38.733 --> 00:15:42.833
可能会将Joe Biden这样的两个单词连接在一起

00:15:43.000 --> 00:15:46.300
诸如这类的注意力头经常会按照顺序来操作

00:15:46.466 --> 00:15:49.033
一个注意力层中的注意力操作结果

00:15:49.033 --> 00:15:52.299
会成为下一层中的另一个注意力头的输入

00:15:52.900 --> 00:15:53.500
事实上呢

00:15:53.500 --> 00:15:56.866
我们刚才列举的每个任务可能都需要多个注意力头

00:15:56.866 --> 00:15:58.233
而不仅仅是一个

00:15:58.966 --> 00:16:01.700
GPT-3的最大版本呢有96个层

00:16:01.700 --> 00:16:03.966
每个层有96个注意力头

00:16:04.200 --> 00:16:06.800
因此每次预测一个新词的时候

00:16:07.000 --> 00:16:10.966
GPT-3将执行9,216个注意力的操作

00:16:11.566 --> 00:16:12.300
以上内容

00:16:12.300 --> 00:16:16.033
我们展示了注意力头工作的方式的一个理想化的版本

00:16:16.466 --> 00:16:21.366
现在让我们来看一下关于真实语言模型内部运作的研究

00:16:21.533 --> 00:16:25.566
去年研究人员在Redwood research研究了GPT-2

00:16:25.700 --> 00:16:27.266
即ChatGPT的前身

00:16:27.800 --> 00:16:32.500
对于“When Mary and John went to the store, John gave a drink to”

00:16:32.600 --> 00:16:36.800
这个段落翻译过来就是当玛丽和约翰去商店

00:16:36.800 --> 00:16:38.866
约翰把一杯饮料给了

00:16:39.066 --> 00:16:41.166
预测这句话下一个单词的过程

00:16:41.900 --> 00:16:44.933
GPT-2预测的下一个单词呢是Mary玛丽

00:16:45.133 --> 00:16:46.200
研究人员就发现

00:16:46.200 --> 00:16:49.700
有三种类型的注意力头对这个预测做出了贡献

00:16:49.933 --> 00:16:50.733
第一种

00:16:50.766 --> 00:16:53.700
三个被他们称为名称移动头的注意力头

00:16:53.833 --> 00:16:55.100
（Name Mover Head）

00:16:55.300 --> 00:16:58.933
将信息呢从Marry向量复制到了最后的输入向量

00:16:59.100 --> 00:17:01.600
也就是to这个词所对应的向量

00:17:01.766 --> 00:17:06.233
GPT-2使用这个最右向量中的信息来预测下一个单词

00:17:06.233 --> 00:17:10.066
那么神经网络又是如何来决定Marry是正确的复制词呢

00:17:10.466 --> 00:17:13.466
通过GPT-2的计算过程进行逆向的推导

00:17:13.666 --> 00:17:18.300
科学家们发现了一组他们称之为主语抑制头的四个注意力头（Subject Inhibition Head）

00:17:18.466 --> 00:17:21.166
它们标记了第二个John向量

00:17:21.300 --> 00:17:24.333
阻止了名称移动头来复制John这个名字

00:17:24.533 --> 00:17:27.933
主语抑制头又是如何知道不应该复制John的呢

00:17:28.366 --> 00:17:29.833
团队进一步向后推导

00:17:29.933 --> 00:17:33.833
发现了他们称为重复标记头的两个注意力头

00:17:34.033 --> 00:17:35.533
（Duplicate Token Heads）

00:17:35.700 --> 00:17:37.000
他们将第二个John向量

00:17:37.000 --> 00:17:39.566
标记为第一个John向量的重复副本

00:17:39.900 --> 00:17:43.033
这帮助主语抑制头来决定不应该复制John

00:17:43.400 --> 00:17:44.366
简而言之

00:17:44.400 --> 00:17:47.300
这9个注意力头使得GPT-2能够理解

00:17:47.400 --> 00:17:50.100
“John gave a drink to John”是没有意义的

00:17:50.200 --> 00:17:53.100
而选择“John gave a drink to Mary”

00:17:53.100 --> 00:17:54.800
这个例子呢也侧面说明了

00:17:54.800 --> 00:17:57.633
要完全理解大语言模型会有多么困难

00:17:57.800 --> 00:18:00.266
由五位研究人员组成的Redwood团队

00:18:00.266 --> 00:18:03.066
曾经发表了一篇25页的论文

00:18:03.333 --> 00:18:06.866
解释了他们是如何识别和验证这些注意力头的

00:18:07.266 --> 00:18:09.500
然而即使他们完成了所有这些工作

00:18:09.666 --> 00:18:12.266
我们离对于为什么GPT-2决定

00:18:12.266 --> 00:18:14.933
预测Mary作为下一个单词的全面解释

00:18:15.000 --> 00:18:16.400
还有很长的路要走

00:18:16.833 --> 00:18:21.033
比如说模型是如何知道下一个单词应该是某个人的名字

00:18:21.033 --> 00:18:22.966
而不是其他类型的单词

00:18:23.133 --> 00:18:25.133
很容易想到在类似的句子中

00:18:25.300 --> 00:18:27.766
Mary不会是一个好的下一个预测词

00:18:27.966 --> 00:18:28.366
比如说

00:18:28.366 --> 00:18:35.300
在句子“when Mary and John went to the restaurant, John gave his keys to”这个句子中

00:18:35.300 --> 00:18:37.900
逻辑上呢下一个词应该是“the valet”

00:18:38.033 --> 00:18:39.533
即代客停车员

00:18:40.033 --> 00:18:42.899
假设计算机科学家们进行了充足的研究

00:18:43.033 --> 00:18:45.033
也许他们可以揭示和解释

00:18:45.166 --> 00:18:47.300
GPT-2推理过程中的其他步骤

00:18:47.533 --> 00:18:50.799
最终呢他们可能能够全面理解GPT-2

00:18:50.800 --> 00:18:54.166
是如何决定Marry是句子最可能的下一个单词

00:18:54.500 --> 00:18:57.966
但是这可能需要数个月甚至数年的努力

00:18:58.066 --> 00:19:00.766
才能够理解一个单词的预测情况

00:19:00.966 --> 00:19:03.166
而ChatGPT背后的语言模型

00:19:03.266 --> 00:19:07.600
GPT-3和GPT-4 比GPT-2呢更加的庞大和复杂

00:19:07.900 --> 00:19:10.766
相比于Redwood团队研究的简单句子

00:19:10.833 --> 00:19:13.166
他们能够完成更复杂的推理任务

00:19:13.566 --> 00:19:17.366
因此完全解释这些系统的工作将是一个巨大的项目

00:19:17.800 --> 00:19:20.000
人类不太可能在短时间内完成

00:19:20.200 --> 00:19:22.200
我们继续回到注意力头的部分

00:19:22.433 --> 00:19:25.633
当注意力头在词向量之间传输信息之后

00:19:25.866 --> 00:19:30.500
前馈网络会思考每个词向量并且尝试预测下一个词

00:19:30.900 --> 00:19:34.133
在这个阶段单词之间没有交换任何的信息

00:19:34.333 --> 00:19:36.866
前馈层会独立的去分析每个单词

00:19:37.300 --> 00:19:41.500
但是前馈层可以访问之前由注意力头复制的任何信息

00:19:41.733 --> 00:19:45.333
这个是GPT-3最大版本的前馈层结构

00:19:45.533 --> 00:19:48.899
其中绿色和紫色的圆圈表示神经元

00:19:49.000 --> 00:19:52.266
他们是计算其输入加权和的数学函数

00:19:52.900 --> 00:19:55.933
前馈层之所以强大是因为它有大量的连接

00:19:56.300 --> 00:19:59.733
在图上呢我们使用了三个神经元作为输出层

00:19:59.966 --> 00:20:01.900
六个神经元作为隐藏层

00:20:01.933 --> 00:20:03.266
绘制出了这个网络

00:20:03.500 --> 00:20:06.500
但是GPT-3的前馈层要大得多

00:20:06.900 --> 00:20:10.333
它的输出层有12,288个神经元

00:20:10.466 --> 00:20:13.833
对应模型的12,288维的词向量

00:20:14.433 --> 00:20:17.800
每个神经元有49,152个输入值

00:20:17.933 --> 00:20:22.366
也就是每个神经元有49,152个权重参数

00:20:22.566 --> 00:20:25.966
而隐藏层呢有49,152个神经元

00:20:26.166 --> 00:20:29.333
每个神经元呢有12,288个输入值

00:20:29.466 --> 00:20:33.466
也就是每个神经元有12,288个权重参数

00:20:33.933 --> 00:20:38.866
这意味着每个前馈层有49,152乘以12,288

00:20:39.066 --> 00:20:43.166
再加上12,288乘以49,152个

00:20:43.233 --> 00:20:45.533
约等于12亿个权重参数

00:20:45.766 --> 00:20:47.633
并且有96个前馈层

00:20:47.700 --> 00:20:52.466
那加起来就是12亿乘以96等于1,160亿个参数

00:20:52.833 --> 00:20:58.499
这相当于具有1,750亿参数的GPT-3将近2/3的参数量

00:20:58.966 --> 00:21:00.933
在2020年的一篇论文中

00:21:00.933 --> 00:21:03.433
来自特拉维夫大学的研究人员就发现

00:21:03.633 --> 00:21:06.333
前馈层通过模式匹配进行工作

00:21:06.666 --> 00:21:08.733
即隐藏层中的每个神经元

00:21:08.733 --> 00:21:11.433
都能够匹配输入文本中的特定模式

00:21:11.633 --> 00:21:14.300
下面呢是一个16层版本的GPT-2中

00:21:14.333 --> 00:21:16.133
一些神经元匹配的模式

00:21:16.366 --> 00:21:20.600
第一层的神经元匹配以substitutes结尾的词序列

00:21:20.766 --> 00:21:23.066
第6层的神经元匹配与军事有关

00:21:23.200 --> 00:21:26.233
并且以base或者bases结尾的词序列

00:21:26.500 --> 00:21:30.600
第13层的神经元匹配以时间范围结尾的序列

00:21:30.700 --> 00:21:33.100
比如说在下午3点到7点之间

00:21:33.200 --> 00:21:35.866
或者从周五晚上7点到

00:21:36.600 --> 00:21:40.166
第16层的神经元匹配与电视节目相关的序列

00:21:40.166 --> 00:21:43.400
比如说原始的NBC日间版本已存档

00:21:43.566 --> 00:21:47.699
或者说时间延迟使该集的观众增加了57%

00:21:48.166 --> 00:21:50.300
没错正如我们所看到的

00:21:50.300 --> 00:21:53.600
越是在后面的层中模式会变得越来抽象

00:21:53.766 --> 00:21:56.600
早期的层会倾向于匹配特定的单词

00:21:56.800 --> 00:22:00.600
而后期的层则匹配属于更广泛语言类别的短语

00:22:00.733 --> 00:22:03.300
比如说电视节目或者说时间间隔

00:22:03.733 --> 00:22:05.433
这部分呢其实很有意思

00:22:05.733 --> 00:22:08.033
因为正如我们之前所说的

00:22:08.133 --> 00:22:11.300
前馈层呢每次只能检查一个单词

00:22:11.433 --> 00:22:15.366
因此当将训练原始的NBC日间版本已存档

00:22:15.400 --> 00:22:17.366
分类为与电视相关的时候

00:22:17.600 --> 00:22:20.100
他只能访问已存档这个词的向量

00:22:20.100 --> 00:22:22.833
而不是NBC或者是日间等等词汇

00:22:23.500 --> 00:22:26.733
可以推断出前馈层之所以可以判断已存档

00:22:26.733 --> 00:22:29.600
是电视节目相关序列的一部分

00:22:29.833 --> 00:22:32.933
是因为注意力头之前已经将上下文的信息

00:22:32.933 --> 00:22:36.033
移到了已存档archived的这个词的向量中

00:22:36.600 --> 00:22:39.366
当一个神经元与其中一个模式匹配的时候

00:22:39.433 --> 00:22:42.133
他就会向这些词像量中添加信息

00:22:42.500 --> 00:22:45.300
虽然这些信息呢并不总是很容易解释的

00:22:45.433 --> 00:22:46.800
但是在许多情况下

00:22:46.933 --> 00:22:50.000
你可以将它视为对下一个词的临时的预测

00:22:50.133 --> 00:22:53.266
我们之前讨论过Google的word2vec的研究

00:22:53.266 --> 00:22:56.033
它可以使用向量运算来进行类比的推理

00:22:56.133 --> 00:22:59.766
比如说柏林减去德国加上法国等于巴黎

00:22:59.900 --> 00:23:02.466
布朗大学的研究人员就发现前馈层

00:23:02.466 --> 00:23:06.133
有时候会使用这种准确的方法来预测下一个单词

00:23:06.433 --> 00:23:09.899
比如说他们研究了GPT-2对以下提示的回应

00:23:10.133 --> 00:23:13.933
问题法国的首都是什么回答巴黎

00:23:14.366 --> 00:23:17.533
问题波兰的首都是什么回答华沙

00:23:18.266 --> 00:23:22.433
这个团队研究了一个包含24层的GPT-2的版本

00:23:22.533 --> 00:23:23.666
在每个层之后

00:23:23.666 --> 00:23:26.100
布朗大学的科学家们去探测模型

00:23:26.133 --> 00:23:28.833
观察他对下一个token的最佳预测

00:23:29.333 --> 00:23:32.433
在前15层最高的可能性的猜测

00:23:32.466 --> 00:23:34.633
是一个看似于随机的单词

00:23:35.100 --> 00:23:37.533
在第16层和第19层之间

00:23:37.633 --> 00:23:40.300
模型开始预测下一个单词是波兰

00:23:40.533 --> 00:23:43.166
不正确但是越来越接近于正确

00:23:43.533 --> 00:23:47.699
然后在第20层最高可能性的猜测变成华沙

00:23:47.800 --> 00:23:48.833
这是正确的答案

00:23:48.900 --> 00:23:51.000
并且在最后4层保持不变

00:23:51.166 --> 00:23:54.500
布朗大学的研究人员发现第20个前馈层

00:23:54.500 --> 00:23:58.800
通过添加了一个将国家向量映射到其对应首都的向量

00:23:58.800 --> 00:24:01.133
从而将波兰转化为了华沙

00:24:01.633 --> 00:24:05.300
将相同的向量添加到中国时候答案会得到北京

00:24:05.900 --> 00:24:08.966
同一个模型中的前馈层会使用向量运算

00:24:09.000 --> 00:24:11.500
将小写单词转换为大写单词

00:24:11.533 --> 00:24:15.133
并将现在时的单词转换为过去时的等效词

00:24:15.733 --> 00:24:16.566
到目前为止呢

00:24:16.566 --> 00:24:17.566
我们已经看了

00:24:17.566 --> 00:24:20.433
GPT-2单词预测的两个实际的示例

00:24:20.733 --> 00:24:24.700
注意力头来帮助预测约翰给玛丽一杯饮料

00:24:24.833 --> 00:24:28.300
而前馈层帮助预测华沙是波兰的首都

00:24:28.499 --> 00:24:29.566
在第一个案例中

00:24:29.566 --> 00:24:31.766
玛丽来自于用户提供的提示

00:24:32.166 --> 00:24:33.466
但是在第二个案例中

00:24:33.466 --> 00:24:35.466
华沙并没有出现在提示中

00:24:35.699 --> 00:24:39.399
相反GPT-2必须记住华沙是波兰的首都

00:24:39.766 --> 00:24:42.299
而这个信息呢是从训练数据中学到的

00:24:42.433 --> 00:24:45.266
当布朗大学的研究人员禁用了

00:24:45.366 --> 00:24:47.633
将波兰转化为华沙的前馈层时

00:24:47.966 --> 00:24:50.566
模型就不再预测下一个词是华沙了

00:24:50.866 --> 00:24:52.000
但是有趣的是

00:24:52.033 --> 00:24:54.866
如果他们接着在提示的开头加上句子

00:24:54.866 --> 00:24:56.800
波兰的首都是华沙

00:24:57.000 --> 00:24:59.933
那么GPT2就能够再次回答这个问题

00:25:00.066 --> 00:25:02.966
这可能是因为GPT2使用的注意力机制

00:25:03.033 --> 00:25:05.533
从提示中提取到了华沙这个名字

00:25:05.700 --> 00:25:08.300
这种分工会更广泛的表示为

00:25:08.366 --> 00:25:11.899
注意力机制从提示的教导部分检索信息

00:25:12.066 --> 00:25:14.933
而前馈层让语言模型能够记住

00:25:14.933 --> 00:25:17.166
没有在提示中出现的信息

00:25:17.600 --> 00:25:19.466
事实上你可以将前馈层

00:25:19.466 --> 00:25:22.833
视为模型从训练数据中学到的信息的数据库

00:25:22.966 --> 00:25:27.199
靠前的前馈层更可能编码与特定单词相关的简单事实

00:25:27.466 --> 00:25:28.066
例如说

00:25:28.066 --> 00:25:31.099
特朗普经常出现在唐纳德这个词之后

00:25:31.633 --> 00:25:35.199
靠后的层则编码会更加复杂的关系

00:25:35.299 --> 00:25:39.033
比如说加入这个向量来将国家转换为他的首都

00:25:39.699 --> 00:25:43.099
以上呢我们就已经详细讲解了大语言模型的推理过程

00:25:43.233 --> 00:25:45.899
接下来啊我们再讲一讲他的训练方式

00:25:46.499 --> 00:25:48.266
许多早期的机器学习算法

00:25:48.366 --> 00:25:50.799
都需要人工来标记训练示例

00:25:51.033 --> 00:25:54.299
比如说训练数据呢可能是带有人工标签

00:25:54.399 --> 00:25:56.999
狗或者猫的一些猫狗的照片

00:25:57.233 --> 00:25:59.599
而正是需要标记数据的需求

00:25:59.599 --> 00:26:02.299
使得人们想要创建足够大的数据集

00:26:02.299 --> 00:26:06.233
来训练强大的模型这件事变得困难而且昂贵

00:26:06.799 --> 00:26:08.766
大语言模型的一个关键的创新之处

00:26:08.766 --> 00:26:11.766
就在于他们不需要显式的标记数据

00:26:12.066 --> 00:26:15.766
相反呢他们通过尝试预测文本段落中的下一个单词

00:26:15.766 --> 00:26:18.633
来学习几乎任何的书面材料

00:26:18.633 --> 00:26:20.666
都可以用来训练这些模型

00:26:20.833 --> 00:26:23.366
从维基百科的页面到新闻文章

00:26:23.366 --> 00:26:24.766
再到计算机的代码

00:26:24.933 --> 00:26:25.866
举个例子来说

00:26:25.866 --> 00:26:28.066
单元模型可能会拿到一个输入

00:26:28.133 --> 00:26:31.466
i like my coffee with cream and 某某

00:26:31.766 --> 00:26:34.933
并且试图预测sugar糖作为下一个单词

00:26:35.433 --> 00:26:37.033
一个新的初始化语言模型

00:26:37.033 --> 00:26:38.866
在这方面表现的很糟糕

00:26:39.033 --> 00:26:44.166
因为他的每个权重参数最初基本上都是从一个随机的数字开始的

00:26:44.333 --> 00:26:46.633
但是随着模型看到更多的例子

00:26:46.666 --> 00:26:48.933
比方说数千亿个单词

00:26:49.033 --> 00:26:51.033
这些权重会逐渐的调整

00:26:51.166 --> 00:26:52.733
从而做出更好的预测

00:26:53.066 --> 00:26:55.600
直到像GPT-3最强大的版本一样

00:26:55.600 --> 00:26:57.833
最后达到1,750亿个参数

00:26:57.966 --> 00:27:01.333
下面呢我们用一个类比来说明这个过程是如何进行的

00:27:01.733 --> 00:27:02.866
假设你要洗澡

00:27:02.866 --> 00:27:05.833
希望水温刚刚好不太热也不太冷

00:27:06.066 --> 00:27:08.166
你以前呢从来没有用过这个水龙头

00:27:08.300 --> 00:27:12.300
所以你随意的去调整水龙头把手的这个方向啊

00:27:12.300 --> 00:27:13.666
并触摸水的温度

00:27:13.666 --> 00:27:15.233
如果太热或者太冷

00:27:15.233 --> 00:27:17.733
你就会往相反的方向去转动把手

00:27:18.166 --> 00:27:19.766
当接近适当的水温时候

00:27:19.766 --> 00:27:22.333
你对把手所做的调整幅度呢就越小

00:27:22.700 --> 00:27:25.566
现在让我们来对这个类比做几个改动

00:27:25.900 --> 00:27:29.900
首先你想象一下有50,257个水龙头

00:27:30.100 --> 00:27:32.533
每个水龙头对应着一个不同的单词

00:27:32.533 --> 00:27:35.466
比如说the cat或者是bank

00:27:35.833 --> 00:27:39.066
你的目标是只让与序列中下一个单词

00:27:39.133 --> 00:27:41.033
相对应的水龙头里出水

00:27:41.333 --> 00:27:44.299
其次水龙头后面有一大堆互连的管道

00:27:44.466 --> 00:27:46.800
并且这些管道上呢还有一堆阀门

00:27:47.200 --> 00:27:50.500
所以呢如果水从错误的水龙头里流出来了

00:27:50.633 --> 00:27:53.266
你不能只是调整水龙头上的这个旋钮

00:27:53.366 --> 00:27:57.066
你要派遣一只聪明的松鼠部队去追踪每条管道

00:27:57.166 --> 00:28:00.266
并且沿途呢去调整他们找到的每个阀门

00:28:00.433 --> 00:28:01.933
这样就会变得很复杂了

00:28:02.133 --> 00:28:04.999
由于同一个管道经常会供应多个水龙头

00:28:05.066 --> 00:28:10.399
所以需要仔细的思考如何确定要拧紧或者松开哪些阀门

00:28:10.600 --> 00:28:12.766
以及到底拧多大程度

00:28:12.899 --> 00:28:15.533
显然如果我们仅仅从字面上来理解

00:28:15.533 --> 00:28:17.666
这个例子就会变得非常荒谬

00:28:18.266 --> 00:28:21.799
建立一个拥有1,750亿个阀门的管道网络

00:28:21.966 --> 00:28:23.733
既不现实也没有什么用

00:28:23.999 --> 00:28:25.299
但是由于摩尔定律

00:28:25.299 --> 00:28:28.966
计算机可以并且确实在以这种规模在运行

00:28:29.133 --> 00:28:30.100
截止到目前

00:28:30.200 --> 00:28:32.600
我们所讨论的大语言模型的所有部分

00:28:32.733 --> 00:28:34.600
包括前馈层的神经元

00:28:34.633 --> 00:28:37.700
以及在单词之间传递上下文信息的注意力头

00:28:38.166 --> 00:28:40.600
都被实现为了一系列简单的数学函数

00:28:40.633 --> 00:28:42.766
其中呢主要是矩阵乘法

00:28:43.033 --> 00:28:46.066
它的行为由可调整的权重参数来确定

00:28:46.433 --> 00:28:50.366
就像我故事中的松鼠来松紧阀门控制水流一样

00:28:50.766 --> 00:28:55.433
训练算法是通过增加或者减少语言模型的权重参数

00:28:55.566 --> 00:28:57.966
来控制信息在神经网络中的流动

00:28:58.466 --> 00:29:00.133
训练过程分为两个步骤

00:29:00.266 --> 00:29:02.933
首先进行前向传播forward pass

00:29:03.200 --> 00:29:04.033
打开水源

00:29:04.033 --> 00:29:07.466
并且检查水是否从正确的水龙头中流出

00:29:07.600 --> 00:29:08.833
然后关闭水源

00:29:08.933 --> 00:29:11.499
进行反向传播backwards pass

00:29:11.633 --> 00:29:14.300
松鼠们就沿着每根管道飞快的奔跑

00:29:14.300 --> 00:29:16.100
拧紧或者松开这个阀门

00:29:16.633 --> 00:29:18.500
在数字化的神经网络中

00:29:18.533 --> 00:29:22.466
松鼠的角色是由一个被称为反向传播的算法来扮演

00:29:22.566 --> 00:29:24.766
这个算法会逆向的通过网络

00:29:24.899 --> 00:29:29.066
使用微积分来评估需要改变每个权重参数的过程

00:29:29.566 --> 00:29:31.499
对一个示例进行前向传播

00:29:31.566 --> 00:29:33.233
然后再进行后向传播

00:29:33.366 --> 00:29:35.799
来提高网络在这个示例上的性能

00:29:35.999 --> 00:29:39.199
完成这个过程需要进行数百亿次的数学运算

00:29:39.499 --> 00:29:41.833
而像GPT-3这种大模型的训练

00:29:41.966 --> 00:29:44.066
需要重复这个过程数十亿次

00:29:44.166 --> 00:29:46.966
因为对每个训练数据的每个词都要训练

00:29:47.166 --> 00:29:49.500
OpenAI估计训练GPT-3

00:29:49.500 --> 00:29:52.633
需要超过3,000亿万亿次的浮点计算

00:29:52.800 --> 00:29:57.066
这需要几十个高端的GPU芯片运行数个月才能够完成

00:29:57.566 --> 00:30:01.533
你可能会对训练过程能够如此出色的工作感到很惊讶

00:30:01.933 --> 00:30:04.866
因为ChatGPT可以执行各种复杂的任务

00:30:04.866 --> 00:30:09.033
包括撰写文章进行类比甚至编写计算机代码

00:30:09.200 --> 00:30:11.600
那么这样一个简单的学习机制

00:30:11.733 --> 00:30:14.333
是如何产生如此强大的模型呢

00:30:14.533 --> 00:30:15.933
一个原因呢是规模

00:30:16.100 --> 00:30:20.100
像GPT3这样的模型看到的示例数量是非常之多的

00:30:20.566 --> 00:30:24.366
GPT3呢是在大约5,000亿个单词的语料库上进行训练的

00:30:24.766 --> 00:30:25.566
相比之下

00:30:25.566 --> 00:30:28.333
一个普通的人类孩子在10岁之前

00:30:28.433 --> 00:30:30.733
遇到的单词数量大约是1亿个

00:30:31.100 --> 00:30:32.366
在过去的五年中

00:30:32.533 --> 00:30:35.499
OpenAI不断的增大他的大语言模型的规模

00:30:35.666 --> 00:30:38.499
在一篇广为流传的2020年的论文中

00:30:38.899 --> 00:30:39.899
OpenAI报告称

00:30:40.066 --> 00:30:44.066
他们的语言模型的准确性与语言规模数据集规模

00:30:44.099 --> 00:30:47.366
以及用于训练的计算量呈幂率关系

00:30:47.733 --> 00:30:50.533
一些趋势呢甚至跨越7个数量级以上

00:30:51.166 --> 00:30:52.099
模型规模越大

00:30:52.099 --> 00:30:54.499
在涉及语言的任务上表现的越好

00:30:54.733 --> 00:30:59.533
但是前提是他们需要以类似的倍数来增加训练数据量

00:30:59.633 --> 00:31:02.333
而且要在更多的数据上训练更大的模型

00:31:02.466 --> 00:31:03.700
还需要更多的算力

00:31:03.866 --> 00:31:07.400
2018年OpenAI发布了第一个大模型GPT-1

00:31:07.600 --> 00:31:11.000
它使用了768维的词向量共有12层

00:31:11.100 --> 00:31:13.066
总共有1.17亿个参数

00:31:13.200 --> 00:31:14.000
几个月后

00:31:14.066 --> 00:31:15.533
OpenAI发布了GPT-2

00:31:15.733 --> 00:31:18.600
它最大的版本拥有1,600维的词向量

00:31:18.633 --> 00:31:21.466
48层总共有15亿个参数

00:31:21.633 --> 00:31:23.966
2020年OpenAI发布了GPT-3

00:31:24.100 --> 00:31:27.066
它具有12,288维的词向量

00:31:27.066 --> 00:31:30.133
96层总共有1,751个参数

00:31:30.466 --> 00:31:32.866
今年OpenAI发布了GPT-4

00:31:33.066 --> 00:31:35.666
虽然尚没有公布任何的架构细节

00:31:35.766 --> 00:31:39.666
但是业内普遍认为GPT-4比GPT-3要大得多

00:31:40.100 --> 00:31:44.100
每个模型不仅学到了比他较小的前身模型更多的事实

00:31:44.300 --> 00:31:47.366
而且在需要某种形式的抽象推理任务上

00:31:47.433 --> 00:31:48.966
表现出了更好的性能

00:31:49.633 --> 00:31:51.833
比如说我们设想这样一个故事

00:31:52.033 --> 00:31:53.966
一个装满了爆米花的袋子

00:31:54.100 --> 00:31:55.533
袋子里没有巧克力

00:31:55.633 --> 00:31:58.233
但是袋子上的标签写着是巧克力

00:31:58.233 --> 00:31:59.433
而不是爆米花

00:31:59.533 --> 00:32:01.933
一个小孩山姆发现了这个袋子

00:32:02.066 --> 00:32:04.333
他以前从来没有见过这个袋子

00:32:04.333 --> 00:32:06.466
他也看不见袋子里面的东西

00:32:06.900 --> 00:32:08.700
他读到了这个袋子上的标签

00:32:09.033 --> 00:32:10.066
你可能会猜

00:32:10.100 --> 00:32:12.366
山姆相信袋子里面装着巧克力

00:32:12.433 --> 00:32:15.633
并且会惊讶的发现里面其实是爆米花

00:32:16.300 --> 00:32:19.533
心理学家将这种推理他人思维状态的能力研究

00:32:19.533 --> 00:32:23.066
称之为心智理论theory of mind

00:32:23.066 --> 00:32:26.533
大多数人从上小学开始就具备了这种能力

00:32:26.833 --> 00:32:29.300
虽然专家们对于任何非人类的动物

00:32:29.300 --> 00:32:33.300
比如说黑猩猩是否适用于这种心智理论存在分歧

00:32:33.566 --> 00:32:37.766
但是基本的共识是他对人类社会的认知至关重要

00:32:38.066 --> 00:32:39.233
今年的早些时间

00:32:39.366 --> 00:32:44.099
斯坦福大学心理学家米哈尔科兴斯基发表了一项研究

00:32:44.466 --> 00:32:48.699
研究了大圆模型的能力是否能够解决心智理论的任务

00:32:48.833 --> 00:32:52.333
他给各种语言模型阅读了类似刚刚我们讲的那个故事

00:32:52.533 --> 00:32:54.433
然后要求他们完成一个句子

00:32:54.533 --> 00:32:57.566
比如说他相信袋子里面装满了什么

00:32:58.133 --> 00:32:59.833
正确答案呢应该是巧克力

00:33:00.100 --> 00:33:02.066
但是一个不成熟的语言模型

00:33:02.066 --> 00:33:04.733
可能会说成是爆米花或者其他东西

00:33:05.166 --> 00:33:07.933
GPT-1和GPT-2在这个测试中失败了

00:33:08.133 --> 00:33:11.833
但在2020年发布的GPT-3的第一个版本中

00:33:12.033 --> 00:33:14.300
正确率达到了接近于40%

00:33:14.400 --> 00:33:17.966
科辛斯基将模型的性能水平与3岁的儿童相比较

00:33:18.166 --> 00:33:20.933
去年11月份发布的最新版的GPT-3

00:33:20.966 --> 00:33:24.400
将上述问题的正确率提高到了大约90%

00:33:24.566 --> 00:33:26.466
与7岁的儿童相当

00:33:26.966 --> 00:33:31.499
而GPT-4对心智理论问题的回答正确率呢约为95%

00:33:31.666 --> 00:33:32.733
科辛斯基写道

00:33:32.733 --> 00:33:35.133
鉴于这些模型中既没有迹象表明

00:33:35.266 --> 00:33:37.933
心智化能力被有意的设计进去

00:33:38.066 --> 00:33:41.500
也没有研究证明科学家知道如何实现它

00:33:41.666 --> 00:33:45.000
这个能力很可能是自发而且自主的出现的

00:33:45.333 --> 00:33:48.633
这就是模型的语言能力不断增强的一个副产品

00:33:49.000 --> 00:33:50.466
不过呢值得注意的是

00:33:50.666 --> 00:33:54.533
研究人员并不全都认可这些结果证明了心智理论

00:33:54.666 --> 00:33:56.233
比如说有的人发现

00:33:56.333 --> 00:33:58.733
对错误信念任务的微小更改

00:33:58.866 --> 00:34:01.400
会导致GPT-3的性能大大的下降

00:34:01.533 --> 00:34:06.033
而GPT-3在测量心智理论的其他任务中的表现更为不稳定

00:34:06.199 --> 00:34:07.599
正如肖恩写Hans的那样

00:34:07.599 --> 00:34:11.366
成功的表现可能是归于任务中的混淆因素

00:34:11.566 --> 00:34:13.699
这是一种聪明汉斯的效应

00:34:13.833 --> 00:34:15.333
英文呢是clever Hans

00:34:15.533 --> 00:34:17.433
指的是一匹名为汉斯的马

00:34:17.600 --> 00:34:20.066
看似呢能够完成一些简单的智力任务

00:34:20.200 --> 00:34:23.600
但是实际上只是依赖于人们给出的无意识的线索

00:34:23.733 --> 00:34:26.599
只不过这个效应现在是出现了大语言模型上

00:34:26.600 --> 00:34:27.766
而不是马身上

00:34:28.333 --> 00:34:31.966
尽管如此GPT-3在几个衡量心智理论的任务上

00:34:31.966 --> 00:34:33.366
接近于人类的表现

00:34:33.466 --> 00:34:35.566
这在几年前呢是无法想象的

00:34:35.766 --> 00:34:37.400
并且这与更大的模型

00:34:37.400 --> 00:34:42.000
通常在需要高级推理的任务中表现的更好的观点是相一致的

00:34:42.200 --> 00:34:44.566
这只是语言模型表现出的

00:34:44.566 --> 00:34:47.999
自发发展出高级推理能力的众多的例子之一

00:34:48.433 --> 00:34:49.200
今年4月呢

00:34:49.200 --> 00:34:51.066
微软的研究人员发表了一篇论文

00:34:51.066 --> 00:34:55.833
也表示GPT-4展示了通用人工智能的初步诱人的迹象

00:34:56.133 --> 00:34:59.733
即以一种复杂类人的方式去思考的能力

00:34:59.866 --> 00:35:00.500
比方说呢

00:35:00.500 --> 00:35:02.566
一名研究人员要求GPT-4

00:35:02.666 --> 00:35:06.666
使用一种名为TiKZ的晦涩的图形编程语言

00:35:06.666 --> 00:35:07.666
画一只独角兽

00:35:08.066 --> 00:35:09.833
GPT-4回应了几行代码

00:35:09.833 --> 00:35:11.866
然后研究人员将这些代码输入

00:35:11.866 --> 00:35:14.733
TiKZ软件生成的图像呢虽然粗糙

00:35:14.733 --> 00:35:16.266
但是清晰的显示出

00:35:16.333 --> 00:35:19.033
GPT-4对独角兽的外观有一定的理解

00:35:19.133 --> 00:35:20.233
研究人员认为

00:35:20.333 --> 00:35:23.300
GPT-4可能以某种方式从训练数据中

00:35:23.300 --> 00:35:25.133
记住了绘制独角兽的代码

00:35:25.500 --> 00:35:27.966
所以他们给他提出了一个后续的挑战

00:35:28.166 --> 00:35:31.166
他们修改了独角兽的代码移除了头部的角

00:35:31.166 --> 00:35:33.699
并且呢移动了一其他的一些身体部位

00:35:33.833 --> 00:35:37.266
然后他们让GPT-4把独角兽头上的角放回去

00:35:37.799 --> 00:35:39.066
而GPT-4的回应呢

00:35:39.199 --> 00:35:41.966
正是将头上的角放在了正确的位置上

00:35:42.433 --> 00:35:44.766
尽管作者的测试版本和训练数据

00:35:44.766 --> 00:35:46.099
完全是基于文本的

00:35:46.099 --> 00:35:47.833
没有包含任何的图像

00:35:47.966 --> 00:35:50.699
但是GBT-4似乎仍然能够完成这个任务

00:35:51.166 --> 00:35:54.133
不过呢通过大量的书面文本训练之后

00:35:54.333 --> 00:35:58.399
GPT-4显然学会了推理关于独角兽身体形状的知识

00:35:58.733 --> 00:36:02.266
目前呢我们对于大语言模型如何完成这样的壮举

00:36:02.299 --> 00:36:03.566
没有真正的了解

00:36:03.966 --> 00:36:06.299
有些人认为呢像这样的例子表明

00:36:06.366 --> 00:36:09.533
模型开始真正理解训练集中词的含义

00:36:09.899 --> 00:36:11.566
而其他人呢则坚持认为

00:36:11.666 --> 00:36:13.766
语言模型呢只是一种随机鹦鹉

00:36:13.966 --> 00:36:16.933
仅仅是重复越来越复杂的单词序列

00:36:16.933 --> 00:36:18.666
而并非真正理解他们

00:36:18.766 --> 00:36:20.533
那关于什么是随机鹦鹉

00:36:20.666 --> 00:36:22.833
我们找时间也会专门去介绍一下

00:36:23.233 --> 00:36:26.333
这种辩论指向了一种深刻的哲学争论

00:36:26.366 --> 00:36:27.500
可能无法解决

00:36:27.733 --> 00:36:28.433
尽管如此

00:36:28.433 --> 00:36:32.033
我们认为关注GPT-3这些模型的经验表现

00:36:32.166 --> 00:36:33.200
也是很重要的

00:36:33.500 --> 00:36:34.766
如果一个语言模型

00:36:34.766 --> 00:36:38.166
能够在特定类型的问题中始终得到正确的答案

00:36:38.333 --> 00:36:41.599
并且呢研究人员有信心排除掉混淆的因素

00:36:41.700 --> 00:36:46.000
比如说可以确保模型在训练期间没有接触过这些问题

00:36:46.300 --> 00:36:48.700
那么无论他们对语言的理解方式

00:36:48.733 --> 00:36:50.566
是否跟人类完全相同

00:36:50.633 --> 00:36:53.100
这都是一个有趣而且重要的结果

00:36:53.533 --> 00:36:57.566
训练下一个token预测如此有效的另外一个可能的原因

00:36:57.800 --> 00:37:00.300
就是语言本身是可以预测的

00:37:00.466 --> 00:37:04.133
语言的规律性通常会跟物质世界的规律性相关联

00:37:04.366 --> 00:37:07.966
因此当语言模型学习单词之间的关系时候

00:37:08.066 --> 00:37:11.733
通常也在隐含的学习跟这个世界存在的关系

00:37:12.233 --> 00:37:16.233
此外呢预测可能是生物智能以及人工智能的一个基础

00:37:16.366 --> 00:37:18.866
根据Andy Clark等哲学家的观点

00:37:19.066 --> 00:37:21.700
人脑呢可以被认为是一个预测机器

00:37:21.966 --> 00:37:22.866
它的主要任务呢

00:37:22.866 --> 00:37:24.700
是对我们的环境进行预测

00:37:24.833 --> 00:37:27.666
然后利用这些预测来成功的驾驭环境

00:37:28.066 --> 00:37:31.166
预测对于生物智能和人工智能都至关重要

00:37:31.633 --> 00:37:32.266
直观的说

00:37:32.266 --> 00:37:34.633
好的预测离不开良好的表示

00:37:34.633 --> 00:37:37.200
比如说准确的地图比错误的地图

00:37:37.333 --> 00:37:39.833
更有可能帮助人们去更好的导航

00:37:40.166 --> 00:37:42.066
世界是广阔而复杂的

00:37:42.166 --> 00:37:45.999
进行预测有助于生物高效定位和适应这种复杂性

00:37:46.366 --> 00:37:48.000
在构建语言模型方面

00:37:48.033 --> 00:37:49.999
传统上的一个重大的挑战

00:37:50.033 --> 00:37:53.999
就是如何找出最有用的表示不同单词的方式

00:37:54.266 --> 00:37:58.133
特别是因为许多单词的含义很大程度上取决于上下文

00:37:58.633 --> 00:38:00.166
下一个词的预测方法

00:38:00.166 --> 00:38:03.933
使得研究人员能够将其转换成一个经验性的问题

00:38:04.066 --> 00:38:06.666
以此来避开这个棘手的理论难题

00:38:06.999 --> 00:38:07.933
事实证明

00:38:08.066 --> 00:38:10.666
如果我们提供足够的数据和计算能力

00:38:10.666 --> 00:38:14.166
大语言模型能够通过找出最佳的下一个词的预测

00:38:14.199 --> 00:38:16.399
来学习人类语言的运作方式

00:38:16.866 --> 00:38:18.266
它的不足之处在于

00:38:18.266 --> 00:38:20.899
最终得到的系统内部的运作方式

00:38:20.899 --> 00:38:23.166
人类目前还并不能完全的理解

00:38:23.533 --> 00:38:27.599
好了以上就是对大语言模型整个工作原理的一个解释

00:38:27.599 --> 00:38:29.266
不知道大家理解了多少

00:38:29.399 --> 00:38:31.366
整个内容呢大概13,000多字

00:38:31.366 --> 00:38:33.866
光是录制视频就录了一个多小时

00:38:34.099 --> 00:38:37.133
所以还希望大家多多的点赞评论和转发

00:38:37.299 --> 00:38:39.533
也希望这个视频能够帮助到大家

00:38:39.533 --> 00:38:42.333
对现在的大语言模型有一个基础的理解

00:38:42.666 --> 00:38:43.566
感谢大家的观看

00:38:43.566 --> 00:38:44.866
我们下期再见

